{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to generate data from a Gaussian distribution and then estimate a confidence interval (CI) for the data we have. Then we are then going to run the experiment 100 times and see how many times our confidence interval contains the true value of the parameter. Finally, we are going to see how the estimate of the mean and the CI change as we get more and more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all packages we need\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "#check the version of numpy\n",
    "\n",
    "print(np.__version__)\n",
    "\n",
    "#initialize random seed of numpy random number generator with your ID number(\"matricola\")\n",
    "ID_number = # your ID number\n",
    "np.random.seed(ID_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random data points from a Gaussian with given mean and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**: complete the functions below. The first one returns a value taken at random from a normal distribution with the given mean and variance, while the second one uses the first one and returns a numpy array of $num\\_samples$ values taken at random from a normal distribution with the given mean and variance. For the first function, you can use the $random$ package in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_point(mean, variance):\n",
    "    #TO DO: ADD CODE\n",
    "\n",
    "def generate_dataset(mean, variance,num_samples):\n",
    "    #TO DO: ADD CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain maximum likelihood estimate (MLE) of the mean from the data\n",
    "\n",
    "MLE is a statistical approach for finding the parameters that maximize the joint probability of a given dataset assuming a specific parametric probability function.\n",
    "\n",
    "**Note**: MLE essentially assumes a generative model for the data\n",
    "\n",
    "The general approach can be described as:\n",
    "1. given a (training) set of points $S = ((x_1, y_1), . . . , (x_m, y_m))$, we assume each $(x_i , y_i)$ is i.i.d. from some probability distribution with parameters $\\mathbf{\\theta}$; (note that sometimes $S = (x_1,\\dots, x_m)$).\n",
    "2. consider the *likelihood* of the data given the parameters, that is: $\\Pr[S|\\mathbf{\\theta}]$\n",
    "3. consider the *log likelihood* function, that is $L(S;\\mathbf{\\theta}) = \\log\\left(\\Pr[S|\\mathbf{\\theta}]\\right)$\n",
    "4. then the maximum likelihood estimator (MLE) is: $\\hat{\\mathbf{\\theta}} = \\arg\\max_{\\mathbf{\\theta}} L(S;\\mathbf{\\theta})$\n",
    "\n",
    "Let's now derive the MLE estimator for the mean of Normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$.\n",
    "\n",
    "Let $\\mathbf{x}$ be a collection $(x_1,\\dots, x_m)$ of observations from a normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$.\n",
    "\n",
    "Then the log likelihood w.r.t. to the paramters $\\mathbf{\\theta} = (\\mu,\\sigma^2)$ is\n",
    "\\begin{equation}\n",
    "L(\\mathbf{x},\\mathbf{\\theta}) = \\log p_{\\mathbf{x}}(\\mathbf{x}; \\mu) = - \\frac{m}{2}\\log(2\\pi \\sigma^2) - \\frac{1}{2} \\sum_{i=1}^m \\frac{(x_i-\\mu)^2}{\\sigma^2} \n",
    "\\end{equation}\n",
    "\n",
    "To obtain the MLE estimator of $\\mu$ we need to solve $\\frac{d}{d \\mu} L(\\mathbf{x}, \\mathbf{\\theta}) = 0$. It turns out that the MLE estimator is:\n",
    "\\begin{equation}\n",
    "\\hat{\\mu}_m = \\arg \\max_{\\mu} L(\\mathbf{x}, \\mathbf{\\theta}) = \\frac{1}{m}\\sum_{i=1}^m x_i\n",
    "\\end{equation}\n",
    "that is the sample mean!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**: complete the function below so that given a dataset (as a numpy array) returns the MLE for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLE_mean(dataset):\n",
    "    #TO DO: ADD CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Confidence Interval\n",
    "\n",
    "Given a parameter $\\theta$ that we want to estimate from a set $\\mathbf{x}$ of observations, we say that $I(\\mathbf{x})$ is a *confidence interval* of confidence level $1-\\alpha$ for $\\theta$ if \n",
    "\\begin{equation}\n",
    "\\Pr[\\theta\\in I(x)] \\ge 1 − \\alpha\n",
    "\\end{equation}\n",
    "\n",
    "Note that in order to have an *high precision* in locating $\\theta$, we want a *small* $I(X)$.\n",
    "\n",
    "Consider now the example from above where we have data from  Normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$.\n",
    "\n",
    "Let $\\mathbf{x}$ be a collection $(x_1,\\dots, x_m)$ of observations from a normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$. Assume that we know $\\sigma^2$ (things are more complicated when $\\sigma^2$ is not known).\n",
    "\n",
    "An important result shows that for any given confidence level $1 − \\alpha$, the smallest confidence interval for $\\mu$ is symmetric and centered around the MLE estimate $\\hat{\\mu}_m$ for $\\mu$, and in particular it is:\n",
    "\\begin{equation}\n",
    "I(\\mathbf{x}) = \\left[ \\hat{\\mu}_m - z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{m}}, \\hat{\\mu}_m + z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{m}} \\right]\n",
    "\\end{equation}\n",
    "where $z_{\\frac{\\alpha}{2}}$ is the $1 - {\\alpha}{2}$ percentile of a *standard* normal distribution $\\mathcal{N}(0,1)$, that is, the value $z_{\\frac{\\alpha}{2}}$ is such that for a random variable $Y \\sim \\mathcal{N}(0,1)$:\n",
    "\n",
    "$$\n",
    "\\Pr[Y \\le z_{\\frac{\\alpha}{2}}] = 1 - \\frac{\\alpha}{2}\n",
    "$$\n",
    "\n",
    "or, equivalently\n",
    "\n",
    "$$\n",
    "\\Pr[Y \\ge z_{\\frac{\\alpha}{2}}] = \\frac{\\alpha}{2}.\n",
    "$$\n",
    "\n",
    "For example, $z_{\\frac{0.05}{2}}=1.96$ and $z_{\\frac{0.01}{2}}=2.58$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**: complete the function below so that given the MLE estimate $mean\\_hat$ of the mean, the variance $variance$ used to generate the data, the number $num\\_samples$ of samples in the dataset, and a value $\\alpha$, returns the lower bound and the upper bound of CI of confidence level $1-\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI_Normal(mean_hat, variance, num_samples, alpha):\n",
    "    #TO DO: ADD CODE\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put everything together: generate a dataset with the given parameters, estimate the MLE for the mean and the CI, and print the lower bound and the upper bound of the CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for experiment\n",
    "m = 100\n",
    "true_mean = 1.4\n",
    "true_variance = 2.0\n",
    "significance_level = 0.05\n",
    "\n",
    "data = generate_dataset(true_mean, true_variance, m)\n",
    "estimated_mean = MLE_mean(data)\n",
    "lb, ub = CI_Normal(estimated_mean, true_variance, m, significance_level)\n",
    "print(lb)\n",
    "print(ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**: repeat the experiment above 1000 times and print the fraction of times that the CI does not contain the true value of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's repeat the experiment N times and see how many times the confidence interval \n",
    "#we estimate contains the true value of the parameter\n",
    "\n",
    "#parameters for experiment\n",
    "m = 100\n",
    "true_mean = 1.4\n",
    "true_variance = 2.0\n",
    "significance_level = 0.05\n",
    "num_rep = 1000\n",
    "\n",
    "#TO DO: ADD CODE\n",
    "\n",
    "print(\"Fraction of times we did not get a CI containing the correct value: \"+str(fraction_failures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**: Question: what is the relation between $\\alpha$ and the fraction of times the CI contains the true value of the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute CI for increasing number of samples\n",
    "Now we compute a CI after adding 1 sample at the time to a dataset and plot the corresponding values using matplotlib. To generate 1 sample just use the function $generate\\_dataset$ for 1 sample.\n",
    "\n",
    "**TO DO**: complete the code below so that the following cell plots the true value of the mean, the MLE of the mean after the generation of each sample, and the CI after the generation of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for experiment\n",
    "true_mean = 1.4\n",
    "true_variance = 2.0\n",
    "significance_level = 0.05\n",
    "max_num_samples = 1000\n",
    "\n",
    "#list used to keep the points generated up to the current iteration:\n",
    "#data_list[i] = (i+1)-th sample\n",
    "data_list = list()\n",
    "#list of lower bounds of the CI: lbs[i]=lower bound of CI after i+1 samples have\n",
    "#been generated\n",
    "lbs = np.zeros(max_num_samples)\n",
    "#list of upper bounds of the CI: ubs[i]=upper bound of CI after i+1 samples have\n",
    "#been generated\n",
    "ubs = np.zeros(max_num_samples)\n",
    "#list of MLE for the mean: estimated_means[i]=MLE of the mean after i+1 samples have\n",
    "#been generated\n",
    "estimated_means = np.zeros(max_num_samples)\n",
    "#list of number of samples generated: num_samples[i]= number of samples after i+1 \n",
    "#samples have been generated\n",
    "num_samples = np.zeros(max_num_samples)\n",
    "\n",
    "for m in range(max_num_samples):\n",
    "    #TO DO: add code\n",
    "    new_data = #...\n",
    "\n",
    "    data_list.append(new_data[0])\n",
    "    curr_data = np.array(data_list)\n",
    "    \n",
    "    #TO DO: add code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots what is required. **Note**: it assumes that $num\\_samples$, $estimated\\_means$, $ubs$, $lbs$ are defined as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now plot with matplotlib\n",
    "\n",
    "#the following is to have matplotlib graphs included in your notebook, next to the code\n",
    "%matplotlib inline\n",
    "\n",
    "#Matplotlib(https://matplotlib.org):  2D plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#now plot the estimate MLE as a function of m\n",
    "plt.plot(num_samples, estimated_means,'xb')\n",
    "#to obtain control on the axis of the figure, so to customize laberls, limits, etc.\n",
    "ax1 = plt.gca()\n",
    "#set limit on x-axis\n",
    "ax1.set_xlim(0, max_num_samples)\n",
    "#set limit on y-axis\n",
    "ax1.set_ylim(-0.1, 3)\n",
    "#title of the plot\n",
    "plt.title(\"CI's for varying values of number m of samples\")\n",
    "#label of x axis\n",
    "plt.xlabel(\"m\")\n",
    "#label of y axis\n",
    "plt.ylabel('estimated mean')\n",
    "#now plot the correct value of p, just for reference\n",
    "lines = plt.plot([0.0, max_num_samples],[true_mean, true_mean],'r:')\n",
    "#change the width of the plotted line\n",
    "plt.setp(lines, linewidth=2)\n",
    "#plot the values from CI's upper bound\n",
    "lines = plt.plot(num_samples,ubs,'k:')\n",
    "#change the width of the plotted line\n",
    "plt.setp(lines, linewidth=2)\n",
    "#plot the values from CI's lower bound\n",
    "lines = plt.plot(num_samples,lbs,'k:')\n",
    "#change the width of the plotted line\n",
    "plt.setp(lines, linewidth=2)\n",
    "#add the personalized legend\n",
    "plt.legend(['estimated mean','true mean','CI intervals'])\n",
    "#change the size of the figure - if needed\n",
    "#plt.rcParams['figure.figsize'] = (10,10)\n",
    "#show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
